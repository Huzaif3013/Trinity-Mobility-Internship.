import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv("/content/Plant_1_Generation_Data.csv")
wf = pd.read_csv("/content/Plant_1_Weather_Sensor_Data.csv")
df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])
wf['DATE_TIME'] = pd.to_datetime(wf['DATE_TIME'])
dff = pd.merge(
    df,
    wf,
    on='DATE_TIME',
    how='inner'
)
dff.info()
plt.figure(figsize=(15, 5))
plt.plot(df['DATE_TIME'], df['DC_POWER'])
plt.title('Solar Power Generation Over Time')
plt.show()

# 4. Correlation analysis
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# 5. Distribution plots
df.hist(figsize=(15, 10), bins=50)
plt.show()

# Handle missing values
# Option 1: Forward fill (for time series)
df.fillna(method='ffill', inplace=True)

# Option 2: Interpolation
df['AMBIENT_TEMPERATURE'].interpolate(method='linear', inplace=True)

# Option 3: Drop if too many missing
df.dropna(subset=['DC_POWER'], inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Handle outliers (IQR method)
Q1 = df['DC_POWER'].quantile(0.25)
Q3 = df['DC_POWER'].quantile(0.75)
IQR = Q3 - Q1
df = df[~((df['DC_POWER'] < (Q1 - 1.5 * IQR)) | 
          (df['DC_POWER'] > (Q3 + 1.5 * IQR)))]

# Parse datetime
df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])

# Extract temporal features
df['hour'] = df['DATE_TIME'].dt.hour
df['day'] = df['DATE_TIME'].dt.day
df['month'] = df['DATE_TIME'].dt.month
df['year'] = df['DATE_TIME'].dt.year
df['day_of_week'] = df['DATE_TIME'].dt.dayofweek
df['day_of_year'] = df['DATE_TIME'].dt.dayofyear

# Cyclical encoding (important for time)
df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)
df['month_sin'] = np.sin(2 * np.pi * df['month']/12)
df['month_cos'] = np.cos(2 * np.pi * df['month']/12)

# Create lag features (previous values)
df['power_lag_1'] = df['DC_POWER'].shift(1)
df['power_lag_24'] = df['DC_POWER'].shift(24)  # Previous day same hour

# Rolling statistics
df['power_rolling_mean_24'] = df['DC_POWER'].rolling(window=24).mean()
df['power_rolling_std_24'] = df['DC_POWER'].rolling(window=24).std()

# Weather interaction features
df['temp_irradiation'] = df['AMBIENT_TEMPERATURE'] * df['IRRADIATION']
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# For neural networks, use MinMaxScaler
scaler = MinMaxScaler()

# Select features to scale
features_to_scale = ['AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 
                     'IRRADIATION', 'DC_POWER']

df[features_to_scale] = scaler.fit_transform(df[features_to_scale])
# For time series: NO random split!
# Use chronological split

train_size = int(len(df) * 0.8)
train_data = df[:train_size]
test_data = df[train_size:]

print(f"Training data: {train_data.shape}")
print(f"Testing data: {test_data.shape}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Prepare features and target
X_train = train_data[feature_columns]
y_train = train_data['DC_POWER']
X_test = test_data[feature_columns]
y_test = test_data['DC_POWER']

# Train model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R²: {r2:.4f}")

from sklearn.svm import SVR

svr_model = SVR(kernel='rbf', C=100, gamma=0.1)
svr_model.fit(X_train, y_train)
y_pred_svr = svr_model.predict(X_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Prepare sequences for LSTM
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

seq_length = 24  # Use 24 hours to predict next hour
X_train_seq, y_train_seq = create_sequences(train_data[feature_columns].values, seq_length)
X_test_seq, y_test_seq = create_sequences(test_data[feature_columns].values, seq_length)

# Build LSTM model
model = Sequential([
    LSTM(128, activation='relu', return_sequences=True, 
         input_shape=(seq_length, X_train_seq.shape[2])),
    Dropout(0.2),
    LSTM(64, activation='relu', return_sequences=True),
    Dropout(0.2),
    LSTM(32, activation='relu'),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(
    X_train_seq, y_train_seq,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

# Predict
y_pred_lstm = model.predict(X_test_seq)

from tensorflow.keras.layers import GRU

gru_model = Sequential([
    GRU(128, activation='relu', return_sequences=True, 
        input_shape=(seq_length, X_train_seq.shape[2])),
    Dropout(0.2),
    GRU(64, activation='relu'),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1)
])

gru_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
gru_model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32, 
              validation_split=0.2, callbacks=[early_stop])

from tensorflow.keras.layers import Concatenate, Input
from tensorflow.keras.models import Model

# Input layer
input_layer = Input(shape=(seq_length, X_train_seq.shape[2]))

# LSTM branch
lstm_branch = LSTM(64, return_sequences=True)(input_layer)
lstm_branch = Dropout(0.2)(lstm_branch)
lstm_branch = LSTM(32)(lstm_branch)

# GRU branch
gru_branch = GRU(64, return_sequences=True)(input_layer)
gru_branch = Dropout(0.2)(gru_branch)
gru_branch = GRU(32)(gru_branch)

# Concatenate
merged = Concatenate()([lstm_branch, gru_branch])
merged = Dense(32, activation='relu')(merged)
merged = Dropout(0.2)(merged)
output = Dense(1)(merged)

# Create model
hybrid_model = Model(inputs=input_layer, outputs=output)
hybrid_model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
hybrid_model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32,
                 validation_split=0.2, callbacks=[early_stop])

# Predict multiple future timesteps
def forecast_multistep(model, last_sequence, n_steps):
    """
    Forecast n_steps into the future
    """
    predictions = []
    current_sequence = last_sequence.copy()
    
    for _ in range(n_steps):
        # Predict next step
        pred = model.predict(current_sequence.reshape(1, seq_length, -1))
        predictions.append(pred[0, 0])
        
        # Update sequence (slide window)
        current_sequence = np.roll(current_sequence, -1, axis=0)
        current_sequence[-1] = pred  # Add prediction to sequence
    
    return predictions

# Forecast next 24 hours
last_24_hours = X_test_seq[-1]
future_predictions = forecast_multistep(model, last_24_hours, 24)

def walk_forward_validation(data, seq_length, model, n_steps=24):
    """
    Evaluate model using walk-forward validation
    """
    predictions = []
    actuals = []
    
    for i in range(len(data) - seq_length - n_steps):
        # Use data up to current point
        train_seq = data[i:i+seq_length]
        
        # Predict next n_steps
        pred = model.predict(train_seq.reshape(1, seq_length, -1))
        predictions.append(pred[0, 0])
        
        # Actual value
        actuals.append(data[i+seq_length, 0])  # Assuming target is first column
    
    return np.array(predictions), np.array(actuals)

from statsmodels.tsa.seasonal import seasonal_decompose

# Decompose time series
result = seasonal_decompose(train_data['DC_POWER'], model='multiplicative', period=24)

# Plot components
fig, axes = plt.subplots(4, 1, figsize=(15, 10))
result.observed.plot(ax=axes[0], title='Observed')
result.trend.plot(ax=axes[1], title='Trend')
result.seasonal.plot(ax=axes[2], title='Seasonal')
result.resid.plot(ax=axes[3], title='Residual')
plt.tight_layout()
plt.show()

# Use seasonal component as additional feature
train_data['seasonal'] = result.seasonal

def evaluate_model(y_true, y_pred):
    """
    Calculate all relevant metrics
    """
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    
    print(f"Mean Squared Error (MSE): {mse:.6f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.6f}")
    print(f"Mean Absolute Error (MAE): {mae:.6f}")
    print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
    print(f"R² Score: {r2:.6f}")
    
    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}

# Evaluate
metrics = evaluate_model(y_test_seq, y_pred_lstm)

import matplotlib.pyplot as plt

# Plot predictions vs actual
plt.figure(figsize=(15, 6))
plt.plot(y_test_seq[:100], label='Actual', linewidth=2)
plt.plot(y_pred_lstm[:100], label='Predicted', linewidth=2, alpha=0.7)
plt.xlabel('Time Steps')
plt.ylabel('Solar Power Generation')
plt.title('Solar Power Forecasting: Actual vs Predicted')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Residual plot
residuals = y_test_seq - y_pred_lstm.flatten()
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(y_pred_lstm, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')

plt.subplot(1, 2, 2)
plt.hist(residuals, bins=50, edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Distribution')
plt.tight_layout()
plt.show()

from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import GridSearchCV

def create_model(neurons=64, dropout=0.2, learning_rate=0.001):
    model = Sequential([
        LSTM(neurons, activation='relu', return_sequences=True, 
             input_shape=(seq_length, X_train_seq.shape[2])),
        Dropout(dropout),
        LSTM(neurons//2, activation='relu'),
        Dropout(dropout),
        Dense(1)
    ])
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

# Grid search (simplified for demo)
param_grid = {
    'neurons': [64, 128],
    'dropout': [0.2, 0.3],
    'batch_size': [32, 64],
    'epochs': [50]
}

# Note: This is time-consuming, use carefully
# model = KerasRegressor(build_fn=create_model, verbose=0)
# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
# grid_result = grid.fit(X_train_seq, y_train_seq)

# Save Keras model
model.save('solar_forecasting_lstm_model.h5')

# Save scaler
import joblib
joblib.dump(scaler, 'scaler.pkl')

# Load later
from tensorflow.keras.models import load_model
loaded_model = load_model('solar_forecasting_lstm_model.h5')
loaded_scaler = joblib.load('scaler.pkl')

def predict_solar_power(model, scaler, input_data, seq_length=24):
    """
    Predict solar power generation
    
    Args:
        model: Trained model
        scaler: Fitted scaler
        input_data: Recent data (DataFrame)
        seq_length: Sequence length used in training
    
    Returns:
        Predicted power value
    """
    # Prepare data
    scaled_data = scaler.transform(input_data)
    
    # Create sequence
    if len(scaled_data) < seq_length:
        raise ValueError(f"Need at least {seq_length} data points")
    
    sequence = scaled_data[-seq_length:].reshape(1, seq_length, -1)
    
    # Predict
    prediction = model.predict(sequence, verbose=0)
    
    # Inverse transform if needed
    # prediction = scaler.inverse_transform(prediction)
    
    return prediction[0, 0]

# Example usage
recent_data = test_data[feature_columns].tail(24)
predicted_power = predict_solar_power(model, scaler, recent_data)
print(f"Predicted solar power: {predicted_power:.2f} kW")
